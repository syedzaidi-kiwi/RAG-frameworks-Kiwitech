{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "178d2463-5cde-46c1-8a38-f16cf01c8296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-cloud-auth 0.1.4 requires pydantic<2.0, but you have pydantic 2.6.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q groq langchain langchain-core langchain-groq chromadb pypdf gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145e234b-6126-4782-a15f-21cf83cfd62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   0% ▕                ▏    0 B/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   0% ▕                ▏    0 B/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   0% ▕                ▏    0 B/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   0% ▕                ▏    0 B/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   0% ▕                ▏    0 B/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   0% ▕                ▏    0 B/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   0% ▕                ▏    0 B/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   0% ▕                ▏    0 B/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   0% ▕                ▏    0 B/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   0% ▕                ▏    0 B/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   0% ▕                ▏    0 B/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   0% ▕                ▏    0 B/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   0% ▕                ▏    0 B/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   0% ▕                ▏ 115 KB/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   0% ▕                ▏ 1.2 MB/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   1% ▕                ▏ 3.2 MB/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   2% ▕                ▏ 4.3 MB/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   2% ▕                ▏ 6.3 MB/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   3% ▕                ▏ 8.2 MB/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   3% ▕                ▏ 9.0 MB/274 MB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   4% ▕                ▏  10 MB/274 MB  5.5 MB/s     47s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   4% ▕                ▏  11 MB/274 MB  5.5 MB/s     47s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   5% ▕                ▏  12 MB/274 MB  5.5 MB/s     47s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   5% ▕                ▏  14 MB/274 MB  5.5 MB/s     47s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   6% ▕                ▏  16 MB/274 MB  5.5 MB/s     47s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   6% ▕█               ▏  17 MB/274 MB  5.5 MB/s     46s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   7% ▕█               ▏  19 MB/274 MB  5.5 MB/s     46s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   8% ▕█               ▏  21 MB/274 MB  5.5 MB/s     46s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   8% ▕█               ▏  22 MB/274 MB  5.5 MB/s     45s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   9% ▕█               ▏  24 MB/274 MB  5.5 MB/s     45s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...   9% ▕█               ▏  25 MB/274 MB  8.5 MB/s     29s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  10% ▕█               ▏  26 MB/274 MB  8.5 MB/s     29s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  10% ▕█               ▏  28 MB/274 MB  8.5 MB/s     28s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  11% ▕█               ▏  29 MB/274 MB  8.5 MB/s     28s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  11% ▕█               ▏  30 MB/274 MB  8.5 MB/s     28s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  12% ▕█               ▏  32 MB/274 MB  8.5 MB/s     28s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  13% ▕██              ▏  34 MB/274 MB  8.5 MB/s     28s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  13% ▕██              ▏  35 MB/274 MB  8.5 MB/s     28s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  13% ▕██              ▏  36 MB/274 MB  8.5 MB/s     27s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  14% ▕██              ▏  38 MB/274 MB  8.5 MB/s     27s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  14% ▕██              ▏  39 MB/274 MB  8.5 MB/s     27s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  15% ▕██              ▏  41 MB/274 MB   10 MB/s     23s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  16% ▕██              ▏  42 MB/274 MB   10 MB/s     22s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  16% ▕██              ▏  43 MB/274 MB   10 MB/s     22s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  16% ▕██              ▏  44 MB/274 MB   10 MB/s     22s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  17% ▕██              ▏  45 MB/274 MB   10 MB/s     22s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  17% ▕██              ▏  46 MB/274 MB   10 MB/s     22s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  18% ▕██              ▏  48 MB/274 MB   10 MB/s     22s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  19% ▕██              ▏  51 MB/274 MB   10 MB/s     22s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  19% ▕███             ▏  52 MB/274 MB   10 MB/s     21s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  20% ▕███             ▏  53 MB/274 MB   10 MB/s     21s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  20% ▕███             ▏  55 MB/274 MB   10 MB/s     20s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  20% ▕███             ▏  55 MB/274 MB   10 MB/s     20s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  21% ▕███             ▏  58 MB/274 MB   10 MB/s     19s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  22% ▕███             ▏  60 MB/274 MB   10 MB/s     19s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  22% ▕███             ▏  61 MB/274 MB   10 MB/s     19s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  23% ▕███             ▏  63 MB/274 MB   10 MB/s     19s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  24% ▕███             ▏  65 MB/274 MB   10 MB/s     19s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  24% ▕███             ▏  66 MB/274 MB   10 MB/s     19s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  25% ▕███             ▏  68 MB/274 MB   10 MB/s     18s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  26% ▕████            ▏  70 MB/274 MB   10 MB/s     18s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  26% ▕████            ▏  71 MB/274 MB   11 MB/s     16s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  27% ▕████            ▏  73 MB/274 MB   11 MB/s     16s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  28% ▕████            ▏  75 MB/274 MB   11 MB/s     16s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  28% ▕████            ▏  76 MB/274 MB   11 MB/s     16s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  28% ▕████            ▏  78 MB/274 MB   11 MB/s     16s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  29% ▕████            ▏  80 MB/274 MB   11 MB/s     16s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  29% ▕████            ▏  80 MB/274 MB   11 MB/s     16s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  30% ▕████            ▏  81 MB/274 MB   11 MB/s     16s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  30% ▕████            ▏  83 MB/274 MB   11 MB/s     15s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  31% ▕████            ▏  84 MB/274 MB   11 MB/s     15s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  31% ▕████            ▏  85 MB/274 MB   12 MB/s     15s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  31% ▕█████           ▏  85 MB/274 MB   12 MB/s     15s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  31% ▕█████           ▏  85 MB/274 MB   12 MB/s     15s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  32% ▕█████           ▏  87 MB/274 MB   12 MB/s     15s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  33% ▕█████           ▏  89 MB/274 MB   12 MB/s     15s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  33% ▕█████           ▏  90 MB/274 MB   12 MB/s     15s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  34% ▕█████           ▏  92 MB/274 MB   12 MB/s     14s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  34% ▕█████           ▏  94 MB/274 MB   12 MB/s     14s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  35% ▕█████           ▏  95 MB/274 MB   12 MB/s     14s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  35% ▕█████           ▏  96 MB/274 MB   12 MB/s     14s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  36% ▕█████           ▏  98 MB/274 MB   12 MB/s     14s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  36% ▕█████           ▏  99 MB/274 MB   12 MB/s     14s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  37% ▕█████           ▏ 100 MB/274 MB   12 MB/s     14s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  37% ▕█████           ▏ 102 MB/274 MB   12 MB/s     13s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  38% ▕██████          ▏ 103 MB/274 MB   12 MB/s     13s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  38% ▕██████          ▏ 105 MB/274 MB   12 MB/s     13s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  39% ▕██████          ▏ 106 MB/274 MB   12 MB/s     13s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  39% ▕██████          ▏ 107 MB/274 MB   12 MB/s     13s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  40% ▕██████          ▏ 108 MB/274 MB   12 MB/s     13s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  41% ▕██████          ▏ 111 MB/274 MB   12 MB/s     13s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  41% ▕██████          ▏ 112 MB/274 MB   12 MB/s     13s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  42% ▕██████          ▏ 113 MB/274 MB   12 MB/s     12s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  42% ▕██████          ▏ 115 MB/274 MB   12 MB/s     12s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  42% ▕██████          ▏ 116 MB/274 MB   12 MB/s     12s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  43% ▕██████          ▏ 118 MB/274 MB   12 MB/s     12s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  44% ▕███████         ▏ 120 MB/274 MB   12 MB/s     12s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  44% ▕███████         ▏ 121 MB/274 MB   12 MB/s     12s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  45% ▕███████         ▏ 123 MB/274 MB   12 MB/s     12s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  46% ▕███████         ▏ 124 MB/274 MB   12 MB/s     11s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  46% ▕███████         ▏ 125 MB/274 MB   12 MB/s     11s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  46% ▕███████         ▏ 126 MB/274 MB   12 MB/s     11s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  46% ▕███████         ▏ 126 MB/274 MB   14 MB/s     10s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  46% ▕███████         ▏ 127 MB/274 MB   14 MB/s     10s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  47% ▕███████         ▏ 129 MB/274 MB   14 MB/s     10s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  48% ▕███████         ▏ 131 MB/274 MB   14 MB/s     10s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  48% ▕███████         ▏ 132 MB/274 MB   14 MB/s     10s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  49% ▕███████         ▏ 134 MB/274 MB   14 MB/s      9s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  50% ▕████████        ▏ 137 MB/274 MB   14 MB/s      9s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  50% ▕████████        ▏ 137 MB/274 MB   14 MB/s      9s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  51% ▕████████        ▏ 139 MB/274 MB   14 MB/s      9s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  51% ▕████████        ▏ 141 MB/274 MB   14 MB/s      9s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  52% ▕████████        ▏ 142 MB/274 MB   14 MB/s      9s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  53% ▕████████        ▏ 144 MB/274 MB   14 MB/s      8s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  54% ▕████████        ▏ 146 MB/274 MB   14 MB/s      8s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  54% ▕████████        ▏ 147 MB/274 MB   14 MB/s      8s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  55% ▕████████        ▏ 149 MB/274 MB   14 MB/s      8s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  55% ▕████████        ▏ 151 MB/274 MB   14 MB/s      8s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  56% ▕████████        ▏ 152 MB/274 MB   14 MB/s      8s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  56% ▕████████        ▏ 153 MB/274 MB   14 MB/s      8s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  57% ▕█████████       ▏ 155 MB/274 MB   14 MB/s      8s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  57% ▕█████████       ▏ 156 MB/274 MB   14 MB/s      8s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  58% ▕█████████       ▏ 158 MB/274 MB   14 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  58% ▕█████████       ▏ 159 MB/274 MB   14 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  58% ▕█████████       ▏ 159 MB/274 MB   14 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  59% ▕█████████       ▏ 161 MB/274 MB   14 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  59% ▕█████████       ▏ 162 MB/274 MB   14 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  59% ▕█████████       ▏ 162 MB/274 MB   14 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  59% ▕█████████       ▏ 162 MB/274 MB   14 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  60% ▕█████████       ▏ 163 MB/274 MB   14 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  60% ▕█████████       ▏ 163 MB/274 MB   14 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  60% ▕█████████       ▏ 164 MB/274 MB   14 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  60% ▕█████████       ▏ 165 MB/274 MB   13 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  60% ▕█████████       ▏ 165 MB/274 MB   13 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  61% ▕█████████       ▏ 166 MB/274 MB   13 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  61% ▕█████████       ▏ 167 MB/274 MB   13 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  61% ▕█████████       ▏ 167 MB/274 MB   13 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  61% ▕█████████       ▏ 168 MB/274 MB   13 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  62% ▕█████████       ▏ 168 MB/274 MB   13 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  62% ▕█████████       ▏ 169 MB/274 MB   13 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  62% ▕█████████       ▏ 170 MB/274 MB   13 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  62% ▕█████████       ▏ 170 MB/274 MB   13 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  62% ▕█████████       ▏ 170 MB/274 MB   13 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  62% ▕█████████       ▏ 170 MB/274 MB   12 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  62% ▕█████████       ▏ 171 MB/274 MB   12 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  62% ▕█████████       ▏ 171 MB/274 MB   12 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  63% ▕██████████      ▏ 171 MB/274 MB   12 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  63% ▕██████████      ▏ 173 MB/274 MB   12 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  64% ▕██████████      ▏ 174 MB/274 MB   12 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  64% ▕██████████      ▏ 175 MB/274 MB   12 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  64% ▕██████████      ▏ 176 MB/274 MB   12 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  64% ▕██████████      ▏ 176 MB/274 MB   12 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  65% ▕██████████      ▏ 177 MB/274 MB   12 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  65% ▕██████████      ▏ 178 MB/274 MB   11 MB/s      8s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  65% ▕██████████      ▏ 178 MB/274 MB   11 MB/s      8s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  66% ▕██████████      ▏ 180 MB/274 MB   11 MB/s      8s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  66% ▕██████████      ▏ 181 MB/274 MB   11 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  66% ▕██████████      ▏ 181 MB/274 MB   11 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  66% ▕██████████      ▏ 181 MB/274 MB   11 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  66% ▕██████████      ▏ 182 MB/274 MB   11 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  67% ▕██████████      ▏ 183 MB/274 MB   11 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  67% ▕██████████      ▏ 184 MB/274 MB   11 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  67% ▕██████████      ▏ 184 MB/274 MB   11 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  68% ▕██████████      ▏ 185 MB/274 MB   11 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  68% ▕██████████      ▏ 185 MB/274 MB   11 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  68% ▕██████████      ▏ 185 MB/274 MB   11 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  68% ▕██████████      ▏ 186 MB/274 MB   11 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  68% ▕██████████      ▏ 187 MB/274 MB   11 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  69% ▕██████████      ▏ 188 MB/274 MB   11 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  69% ▕███████████     ▏ 189 MB/274 MB   11 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  69% ▕███████████     ▏ 189 MB/274 MB   11 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  69% ▕███████████     ▏ 190 MB/274 MB   11 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  70% ▕███████████     ▏ 191 MB/274 MB   11 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  71% ▕███████████     ▏ 193 MB/274 MB   10 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  71% ▕███████████     ▏ 194 MB/274 MB   10 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  71% ▕███████████     ▏ 194 MB/274 MB   10 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  71% ▕███████████     ▏ 195 MB/274 MB   10 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  72% ▕███████████     ▏ 196 MB/274 MB   10 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  72% ▕███████████     ▏ 196 MB/274 MB   10 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  72% ▕███████████     ▏ 196 MB/274 MB   10 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  72% ▕███████████     ▏ 197 MB/274 MB   10 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  73% ▕███████████     ▏ 199 MB/274 MB   10 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  73% ▕███████████     ▏ 199 MB/274 MB   10 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  73% ▕███████████     ▏ 200 MB/274 MB  9.7 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  73% ▕███████████     ▏ 200 MB/274 MB  9.7 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  73% ▕███████████     ▏ 201 MB/274 MB  9.7 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  74% ▕███████████     ▏ 202 MB/274 MB  9.7 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  74% ▕███████████     ▏ 202 MB/274 MB  9.7 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  74% ▕███████████     ▏ 203 MB/274 MB  9.7 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  74% ▕███████████     ▏ 204 MB/274 MB  9.7 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  75% ▕███████████     ▏ 204 MB/274 MB  9.7 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  75% ▕███████████     ▏ 205 MB/274 MB  9.7 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  75% ▕████████████    ▏ 206 MB/274 MB  9.7 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  75% ▕████████████    ▏ 206 MB/274 MB  9.7 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  76% ▕████████████    ▏ 208 MB/274 MB  9.0 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  76% ▕████████████    ▏ 208 MB/274 MB  9.0 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  76% ▕████████████    ▏ 208 MB/274 MB  9.0 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  76% ▕████████████    ▏ 209 MB/274 MB  9.0 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  77% ▕████████████    ▏ 210 MB/274 MB  9.0 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  77% ▕████████████    ▏ 211 MB/274 MB  9.0 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  77% ▕████████████    ▏ 212 MB/274 MB  9.0 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  78% ▕████████████    ▏ 213 MB/274 MB  9.0 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  78% ▕████████████    ▏ 214 MB/274 MB  9.0 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  79% ▕████████████    ▏ 215 MB/274 MB  9.0 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  79% ▕████████████    ▏ 217 MB/274 MB  8.2 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  79% ▕████████████    ▏ 217 MB/274 MB  8.2 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  79% ▕████████████    ▏ 218 MB/274 MB  8.2 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  80% ▕████████████    ▏ 218 MB/274 MB  8.2 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  80% ▕████████████    ▏ 218 MB/274 MB  8.2 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  80% ▕████████████    ▏ 219 MB/274 MB  8.2 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  81% ▕████████████    ▏ 221 MB/274 MB  8.2 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  81% ▕████████████    ▏ 222 MB/274 MB  8.2 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  82% ▕█████████████   ▏ 223 MB/274 MB  8.2 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  82% ▕█████████████   ▏ 224 MB/274 MB  8.2 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  82% ▕█████████████   ▏ 224 MB/274 MB  7.4 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  82% ▕█████████████   ▏ 225 MB/274 MB  7.4 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  83% ▕█████████████   ▏ 226 MB/274 MB  7.4 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  83% ▕█████████████   ▏ 227 MB/274 MB  7.4 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  83% ▕█████████████   ▏ 228 MB/274 MB  7.4 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  83% ▕█████████████   ▏ 228 MB/274 MB  7.4 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  83% ▕█████████████   ▏ 228 MB/274 MB  7.4 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  84% ▕█████████████   ▏ 229 MB/274 MB  7.4 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  84% ▕█████████████   ▏ 230 MB/274 MB  7.4 MB/s      5s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  84% ▕█████████████   ▏ 230 MB/274 MB  7.4 MB/s      5s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  84% ▕█████████████   ▏ 231 MB/274 MB  7.3 MB/s      5s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  85% ▕█████████████   ▏ 232 MB/274 MB  7.3 MB/s      5s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  85% ▕█████████████   ▏ 232 MB/274 MB  7.3 MB/s      5s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  85% ▕█████████████   ▏ 233 MB/274 MB  7.3 MB/s      5s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  86% ▕█████████████   ▏ 234 MB/274 MB  7.3 MB/s      5s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  86% ▕█████████████   ▏ 234 MB/274 MB  7.3 MB/s      5s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  86% ▕█████████████   ▏ 236 MB/274 MB  7.3 MB/s      5s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  87% ▕█████████████   ▏ 237 MB/274 MB  7.3 MB/s      5s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  87% ▕█████████████   ▏ 238 MB/274 MB  7.3 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  88% ▕██████████████  ▏ 240 MB/274 MB  7.3 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  88% ▕██████████████  ▏ 241 MB/274 MB  7.8 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  88% ▕██████████████  ▏ 241 MB/274 MB  7.8 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  88% ▕██████████████  ▏ 241 MB/274 MB  7.8 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  88% ▕██████████████  ▏ 242 MB/274 MB  7.8 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  89% ▕██████████████  ▏ 242 MB/274 MB  7.8 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  89% ▕██████████████  ▏ 243 MB/274 MB  7.8 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  89% ▕██████████████  ▏ 243 MB/274 MB  7.8 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  89% ▕██████████████  ▏ 243 MB/274 MB  7.8 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  89% ▕██████████████  ▏ 244 MB/274 MB  7.8 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  89% ▕██████████████  ▏ 244 MB/274 MB  7.8 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  90% ▕██████████████  ▏ 245 MB/274 MB  7.8 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  90% ▕██████████████  ▏ 246 MB/274 MB  7.6 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  90% ▕██████████████  ▏ 247 MB/274 MB  7.6 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  90% ▕██████████████  ▏ 247 MB/274 MB  7.6 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  90% ▕██████████████  ▏ 248 MB/274 MB  7.6 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  91% ▕██████████████  ▏ 248 MB/274 MB  7.6 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  91% ▕██████████████  ▏ 248 MB/274 MB  7.6 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  91% ▕██████████████  ▏ 249 MB/274 MB  7.6 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  91% ▕██████████████  ▏ 250 MB/274 MB  7.6 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  91% ▕██████████████  ▏ 250 MB/274 MB  7.6 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  91% ▕██████████████  ▏ 250 MB/274 MB  7.6 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  92% ▕██████████████  ▏ 251 MB/274 MB  7.3 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  92% ▕██████████████  ▏ 251 MB/274 MB  7.3 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  92% ▕██████████████  ▏ 251 MB/274 MB  7.3 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  92% ▕██████████████  ▏ 251 MB/274 MB  7.3 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  92% ▕██████████████  ▏ 251 MB/274 MB  7.3 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  92% ▕██████████████  ▏ 252 MB/274 MB  7.3 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  92% ▕██████████████  ▏ 252 MB/274 MB  7.3 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  92% ▕██████████████  ▏ 252 MB/274 MB  7.3 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  92% ▕██████████████  ▏ 252 MB/274 MB  7.3 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  92% ▕██████████████  ▏ 253 MB/274 MB  7.3 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  92% ▕██████████████  ▏ 253 MB/274 MB  6.6 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  92% ▕██████████████  ▏ 253 MB/274 MB  6.6 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  93% ▕██████████████  ▏ 253 MB/274 MB  6.6 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  93% ▕██████████████  ▏ 254 MB/274 MB  6.6 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  93% ▕██████████████  ▏ 254 MB/274 MB  6.6 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  93% ▕██████████████  ▏ 255 MB/274 MB  6.6 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  93% ▕██████████████  ▏ 255 MB/274 MB  6.6 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  93% ▕██████████████  ▏ 255 MB/274 MB  6.6 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  93% ▕██████████████  ▏ 256 MB/274 MB  6.6 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  94% ▕██████████████  ▏ 256 MB/274 MB  6.6 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  94% ▕███████████████ ▏ 257 MB/274 MB  6.3 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  94% ▕███████████████ ▏ 257 MB/274 MB  6.3 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  94% ▕███████████████ ▏ 258 MB/274 MB  6.3 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  94% ▕███████████████ ▏ 259 MB/274 MB  6.3 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  95% ▕███████████████ ▏ 259 MB/274 MB  6.3 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  95% ▕███████████████ ▏ 260 MB/274 MB  6.3 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  95% ▕███████████████ ▏ 261 MB/274 MB  6.3 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  96% ▕███████████████ ▏ 262 MB/274 MB  6.3 MB/s      1s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  96% ▕███████████████ ▏ 262 MB/274 MB  6.3 MB/s      1s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  96% ▕███████████████ ▏ 263 MB/274 MB  6.3 MB/s      1s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  97% ▕███████████████ ▏ 265 MB/274 MB  6.4 MB/s      1s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  97% ▕███████████████ ▏ 265 MB/274 MB  6.4 MB/s      1s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  97% ▕███████████████ ▏ 267 MB/274 MB  6.4 MB/s      1s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  98% ▕███████████████ ▏ 268 MB/274 MB  6.4 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  98% ▕███████████████ ▏ 269 MB/274 MB  6.4 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  99% ▕███████████████ ▏ 270 MB/274 MB  6.4 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  99% ▕███████████████ ▏ 272 MB/274 MB  6.4 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90...  99% ▕███████████████ ▏ 272 MB/274 MB  6.4 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕███████████████ ▏ 273 MB/274 MB  6.4 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕███████████████ ▏ 273 MB/274 MB  6.4 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕███████████████ ▏ 273 MB/274 MB  6.4 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917...   0% ▕                ▏    0 B/ 11 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917...   0% ▕                ▏    0 B/ 11 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917...   0% ▕                ▏    0 B/ 11 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917...   0% ▕                ▏    0 B/ 11 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917...   0% ▕                ▏    0 B/ 11 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917...   0% ▕                ▏    0 B/ 11 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917...   0% ▕                ▏    0 B/ 11 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917...   0% ▕                ▏    0 B/ 11 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917...   0% ▕                ▏    0 B/ 11 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917...   0% ▕                ▏    0 B/ 11 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917...   0% ▕                ▏    0 B/ 11 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917...   0% ▕                ▏    0 B/ 11 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917...   0% ▕                ▏    0 B/ 11 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046...   0% ▕                ▏    0 B/  17 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046...   0% ▕                ▏    0 B/  17 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046...   0% ▕                ▏    0 B/  17 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046...   0% ▕                ▏    0 B/  17 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046...   0% ▕                ▏    0 B/  17 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046...   0% ▕                ▏    0 B/  17 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046...   0% ▕                ▏    0 B/  17 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046...   0% ▕                ▏    0 B/  17 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046...   0% ▕                ▏    0 B/  17 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046...   0% ▕                ▏    0 B/  17 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046...   0% ▕                ▏    0 B/  17 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046...   0% ▕                ▏    0 B/  17 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046...   0% ▕                ▏    0 B/  17 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa...   0% ▕                ▏    0 B/ 420 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa...   0% ▕                ▏    0 B/ 420 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa...   0% ▕                ▏    0 B/ 420 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa...   0% ▕                ▏    0 B/ 420 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa...   0% ▕                ▏    0 B/ 420 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa...   0% ▕                ▏    0 B/ 420 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa...   0% ▕                ▏    0 B/ 420 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa...   0% ▕                ▏    0 B/ 420 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa...   0% ▕                ▏    0 B/ 420 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa...   0% ▕                ▏    0 B/ 420 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa...   0% ▕                ▏    0 B/ 420 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa...   0% ▕                ▏    0 B/ 420 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa...   0% ▕                ▏    0 B/ 420 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa...   0% ▕                ▏    0 B/ 420 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "removing any unused layers \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a06004f-dae4-4fc6-a60c-7b9fdcd745f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.23.26-cp311-none-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.23.22 (from pymupdf)\n",
      "  Downloading PyMuPDFb-1.23.22-py3-none-macosx_11_0_arm64.whl.metadata (1.4 kB)\n",
      "Downloading PyMuPDF-1.23.26-cp311-none-macosx_11_0_arm64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyMuPDFb-1.23.22-py3-none-macosx_11_0_arm64.whl (29.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
      "Successfully installed PyMuPDFb-1.23.22 pymupdf-1.23.26\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f45fd513-1934-4e42-80d5-aed510d8a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community import embeddings\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "import time\n",
    "import textwrap\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "baf1b72c-0995-4d2a-8189-e38caadebfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in document:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "file_path = '/Users/kiwitech/Downloads/1Bit Quantization in LLMs.pdf'\n",
    "pdf_text = extract_text_from_pdf(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a6980ee-6c39-4876-b4e6-d02eae309070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Era of 1-bit LLMs:\\nAll Large Language Models are in 1.58 Bits\\nShuming Ma∗ Hongyu Wang∗ Lingxiao Ma\\nLei Wang\\nWenhui Wang\\nShaohan Huang\\nLi Dong\\nRuiping Wang\\nJilong Xue\\nFuru Wei⋄\\nhttps://aka.ms/GeneralAI\\nAbstract\\nRecent research, such as BitNet [WMD+23], is paving the way for a new era of 1-\\nbit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,\\nnamely BitNet b1.58, in which every single parameter (or weight) of the LLM is\\nternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer\\nLLM with the same model size and training tokens in terms of both perplexity\\nand end-task performance, while being significantly more cost-effective in terms\\nof latency, memory, throughput, and energy consumption. More profoundly, the\\n1.58-bit LLM defines a new scaling law and recipe for training new generations of\\nLLMs that are both high-performance and cost-effective. Furthermore, it enables\\na new computation paradigm and opens the door for designing specific hardware\\noptimized for 1-bit LLMs.\\n0.2961 -0.0495\\n0.0413\\n...\\n…\\n-0.4765\\n0.2812\\n0.2403\\n-0.1808 0.1304\\n-0.4809\\n…\\n…\\n-0.1771\\n-0.1741 -0.3853\\nTransformer LLMs\\n16-bit Float (FP16/BF16)\\nCost\\nPerformance\\n1\\n-1\\n0\\n…\\n…\\n1\\n-1\\n-1\\n-1\\n1\\n-1\\n…\\n…\\n0\\n0\\n-1\\nBitNet b1.58 (This Work)\\n{-1, 0, 1}\\nW=\\nPareto Improvement\\nW=\\n0.0413\\n0.3397\\n0.2812 0.2403\\n-0.1808 0.1304\\n-0.4809 0.3244\\n0.4322 -0.1771\\n-0.1741 -0.3853\\n0.2961\\n-0.0495 -0.0924 -0.4765\\n𝒙𝟎\\n𝒙𝟏\\n𝒙𝟐\\n𝒙𝟑\\n𝟎. 𝟐𝟗𝟔𝟏𝒙𝟎 − 𝟎. 𝟎𝟒𝟗𝟓𝒙𝟏 − 𝟎. 𝟎𝟗𝟐𝟒𝒙𝟐 − 𝟎. 𝟒𝟕𝟔𝟓𝒙𝟑\\n…\\n1\\n-1\\n0\\n1\\n-1\\n1\\n-1\\n-1\\n-1\\n0\\n-1\\n1\\n1\\n-1\\n1\\n0\\n𝒙𝟎 − 𝒙𝟏 − 𝒙𝟐 + 𝒙𝟑\\n…\\n𝒙𝟎\\n𝒙𝟏\\n𝒙𝟐\\n𝒙𝟑\\n1(.58)-bit\\nFP16\\nModel W\\nInput X\\nOutput Y\\nY = f(W, X)\\nGPU\\nNew \\nHardware \\nFigure 1: 1-bit LLMs (e.g., BitNet b1.58) provide a Pareto solution to reduce inference cost (latency,\\nthroughput, and energy) of LLMs while maintaining model performance. The new computation\\nparadigm of BitNet b1.58 calls for actions to design new hardware optimized for 1-bit LLMs.\\n∗ Equal contribution. ⋄ Corresponding author. S. Ma, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, J. Xue,\\nF. Wei are with Microsoft Research. H. Wang and R. Wang are with University of Chinese Academy of Sciences.\\narXiv:2402.17764v1  [cs.CL]  27 Feb 2024\\n1\\nThe Era of 1-bit LLMs\\nIn recent years, the field of AI has seen a rapid growth in the size and capabilities of Large Language\\nModels (LLMs). These models have demonstrated remarkable performance in a wide range of natural\\nlanguage processing tasks, but their increasing size has posed challenges for deployment and raised\\nconcerns about their environmental and economic impact due to high energy consumption. One\\napproach to address these challenges is to use post-training quantization to create low-bit models\\nfor inference [XLS+23, FAHA23, CCKS23, TCS+24]. This technique reduces the precision of\\nweights and activations, significantly reducing the memory and computational requirements of LLMs.\\nThe trend has been to move from 16 bits to lower bits, such as 4-bit variants [FAHA23, LTT+23].\\nHowever, post-training quantization is sub-optimal, even though it is widely used in industry LLMs.\\nRecent work on 1-bit model architectures, such as BitNet [WMD+23], presents a promising direction\\nfor reducing the cost of LLMs while maintaining their performance. Vanilla LLMs are in 16-bit\\nfloating values (i.e., FP16 or BF16), and the bulk of any LLMs is matrix multiplication. Therefore,\\nthe major computation cost comes from the floating-point addition and multiplication operations. In\\ncontrast, the matrix multiplication of BitNet only involves integer addition, which saves orders of\\nenergy cost for LLMs. As the fundamental limit to compute performance in many chips is power, the\\nenergy savings can also be translated into faster computation.\\nIn addition to computation, the process of transferring model parameters from DRAM to the memory\\nof an on-chip accelerator (e.g., SRAM) can be expensive during inference. There have been attempts\\nto enlarge SRAM to improve throughput, but this introduces significantly higher costs than DRAM.\\nCompared to full-precision models, 1-bit LLMs have a much lower memory footprint from both a\\ncapacity and bandwidth standpoint. This can significantly reduce the cost and time of loading weights\\nfrom DRAM, leading to faster and more efficient inference.\\nIn this work, we introduce a significant 1-bit LLM variant called BitNet b1.58, where every parameter\\nis ternary, taking on values of {-1, 0, 1}. We have added an additional value of 0 to the original 1-bit\\nBitNet, resulting in 1.58 bits in the binary system. BitNet b1.58 retains all the benefits of the original\\n1-bit BitNet, including its new computation paradigm, which requires almost no multiplication\\noperations for matrix multiplication and can be highly optimized. Additionally, it has the same energy\\nconsumption as the original 1-bit BitNet and is much more efficient in terms of memory consumption,\\nthroughput and latency compared to FP16 LLM baselines. Furthermore, BitNet b1.58 offers two\\nadditional advantages. Firstly, its modeling capability is stronger due to its explicit support for feature\\nfiltering, made possible by the inclusion of 0 in the model weights, which can significantly improve\\nthe performance of 1-bit LLMs. Secondly, our experiments show that BitNet b1.58 can match full\\nprecision (i.e., FP16) baselines in terms of both perplexity and end-task performance, starting from a\\n3B size, when using the same configuration (e.g., model size, training tokens, etc.).\\n2\\nBitNet b1.58\\nBitNet b1.58 is based on the BitNet architecture, which is a Transformer that replaces nn.Linear with\\nBitLinear. It is trained from scratch, with 1.58-bit weights and 8-bit activations. Compared to the\\noriginal BitNet, it introduces some modifications that we summarize below.\\nQuantization Function.\\nTo constrain the weights to -1, 0, or +1, we adopt an absmean quantization\\nfunction. It first scales the weight matrix by its average absolute value, and then round each value to\\nthe nearest integer among {-1, 0, +1}:\\nf\\nW = RoundClip( W\\nγ + ϵ, −1, 1),\\n(1)\\nRoundClip(x, a, b) = max(a, min(b, round(x))),\\n(2)\\nγ =\\n1\\nnm\\nX\\nij\\n|Wij|.\\n(3)\\nThe quantization function for activations follows the same implementation in BitNet, except that\\nwe do not scale the activations before the non-linear functions to the range [0, Qb]. Instead, the\\n2\\nModels\\nSize\\nMemory (GB)↓\\nLatency (ms)↓\\nPPL↓\\nLLaMA LLM\\n700M\\n2.08 (1.00x)\\n1.18 (1.00x)\\n12.33\\nBitNet b1.58\\n700M\\n0.80 (2.60x)\\n0.96 (1.23x)\\n12.87\\nLLaMA LLM\\n1.3B\\n3.34 (1.00x)\\n1.62 (1.00x)\\n11.25\\nBitNet b1.58\\n1.3B\\n1.14 (2.93x)\\n0.97 (1.67x)\\n11.29\\nLLaMA LLM\\n3B\\n7.89 (1.00x)\\n5.07 (1.00x)\\n10.04\\nBitNet b1.58\\n3B\\n2.22 (3.55x)\\n1.87 (2.71x)\\n9.91\\nBitNet b1.58\\n3.9B\\n2.38 (3.32x)\\n2.11 (2.40x)\\n9.62\\nTable 1: Perplexity as well as the cost of BitNet b1.58 and LLaMA LLM.\\nModels\\nSize\\nARCe\\nARCc\\nHS\\nBQ\\nOQ\\nPQ\\nWGe\\nAvg.\\nLLaMA LLM\\n700M\\n54.7\\n23.0\\n37.0\\n60.0\\n20.2\\n68.9\\n54.8\\n45.5\\nBitNet b1.58\\n700M\\n51.8\\n21.4\\n35.1\\n58.2\\n20.0\\n68.1\\n55.2\\n44.3\\nLLaMA LLM\\n1.3B\\n56.9\\n23.5\\n38.5\\n59.1\\n21.6\\n70.0\\n53.9\\n46.2\\nBitNet b1.58\\n1.3B\\n54.9\\n24.2\\n37.7\\n56.7\\n19.6\\n68.8\\n55.8\\n45.4\\nLLaMA LLM\\n3B\\n62.1\\n25.6\\n43.3\\n61.8\\n24.6\\n72.1\\n58.2\\n49.7\\nBitNet b1.58\\n3B\\n61.4\\n28.3\\n42.9\\n61.5\\n26.6\\n71.5\\n59.3\\n50.2\\nBitNet b1.58\\n3.9B\\n64.2\\n28.7\\n44.2\\n63.5\\n24.2\\n73.2\\n60.5\\n51.2\\nTable 2: Zero-shot accuracy of BitNet b1.58 and LLaMA LLM on the end tasks.\\nactivations are all scaled to [−Qb, Qb] per token to get rid of the zero-point quantization. This is\\nmore convenient and simple for both implementation and system-level optimization, while introduces\\nnegligible effects to the performance in our experiments.\\nLLaMA-alike Components.\\nThe architecture of LLaMA [TLI+23, TMS+23] has been the de-\\nfacto backbone for open-source LLMs.\\nTo embrace the open-source community, our design\\nof BitNet b1.58 adopts the LLaMA-alike components. Specifically, it uses RMSNorm [ZS19],\\nSwiGLU [Sha20], rotary embedding [SAL+24], and removes all biases. In this way, BitNet b1.58\\ncan be integrated into the popular open-source software (e.g., Huggingface, vLLM [KLZ+23], and\\nllama.cpp2) with minimal efforts.\\n3\\nResults\\nWe compared BitNet b1.58 to our reproduced FP16 LLaMA LLM in various sizes. To ensure a fair\\ncomparison, we pre-trained the models on the RedPajama dataset [Com23] for 100 billion tokens.\\nWe evaluated the zero-shot performance on a range of language tasks, including ARC-Easy [YBS19],\\nARC-Challenge [YBS19], Hellaswag [ZHB+19], Winogrande [SBBC20], PIQA [BZB+19], Open-\\nbookQA [MCKS18], and BoolQ [CLC+19]. We also reported the validation perplexity on the\\nWikiText2 [MXBS16] and C4 [RSR+19] datasets.\\nWe compared the runtime GPU memory and latency of both LLaMA LLM and BitNet b1.58. The\\nresults were measured using the FasterTransformer3 codebase, which is well-optimized for LLM\\ninference latency on GPU devices. The 2-bit kernel from Ladder [WMC+23] is also integrated for\\nBitNet b1.58. We reported the time per output token, as it is the major cost for inference.\\nTable 1 summarizes the perplexity and the cost for BitNet b1.58 and LLaMA LLM. It shows that\\nBitNet b1.58 starts to match full precision LLaMA LLM at 3B model size in terms of perplexity,\\nwhile being 2.71 times faster and using 3.55 times less GPU memory. In particular, BitNet b1.58 with\\na 3.9B model size is 2.4 times faster, consumes 3.32 times less memory, but performs significantly\\nbetter than LLaMA LLM 3B.\\n2https://github.com/ggerganov/llama.cpp\\n3https://github.com/NVIDIA/FasterTransformer\\n3\\n1.3B\\n3B\\n7B\\n13B\\n70B\\nModel Size\\n100\\n101\\n102\\nLatency (ms)\\n1.67x\\n2.71x\\n2.90x\\n3.68x\\n4.10x\\nBitNet b1.58\\nLLaMA\\n1.3B\\n3B\\n7B\\n13B\\n70B\\nModel Size\\n100\\n101\\n102\\nMemory (GB)\\n2.93x\\n3.55x\\n4.40x\\n5.12x\\n7.16x\\nBitNet b1.58\\nLLaMA\\nFigure 2: Decoding latency (Left) and memory consumption (Right) of BitNet b1.58 varying the\\nmodel size.\\nModels\\nSize\\nMax Batch Size\\nThroughput (tokens/s)\\nLLaMA LLM\\n70B\\n16 (1.0x)\\n333 (1.0x)\\nBitNet b1.58\\n70B\\n176 (11.0x)\\n2977 (8.9x)\\nTable 3: Comparison of the throughput between BitNet b1.58 70B and LLaMA LLM 70B.\\nTable 2 reports the detailed results of the zero-shot accuracy on the end tasks. We followed the pipeline\\nfrom lm-evaluation-harness4 to perform the evaluation. The results show that the performance gap\\nbetween BitNet b1.58 and LLaMA LLM narrows as the model size increases. More importantly,\\nBitNet b1.58 can match the performance of the full precision baseline starting from a 3B size. Similar\\nto the observation of the perplexity, the end-task results reveal that BitNet b1.58 3.9B outperforms\\nLLaMA LLM 3B with lower memory and latency cost. This demonstrates that BitNet b1.58 is a\\nPareto improvement over the state-of-the-art LLM models.\\nMemory and Latency\\nWe further scaled up the model size to 7B, 13B, and 70B and evaluated the\\ncost. Figure 2 illustrates the trends of latency and memory, showing that the speed-up increases as the\\nmodel size scales. In particular, BitNet b1.58 70B is 4.1 times faster than the LLaMA LLM baseline.\\nThis is because the time cost for nn.Linear grows with the model size. The memory consumption\\nfollows a similar trend, as the embedding remains full precision and its memory proportion is smaller\\nfor larger models. Both latency and memory were measured with a 2-bit kernel, so there is still room\\nfor optimization to further reduce the cost.\\nEnergy\\nWe also estimate the arithmetic operations energy consumption of both BitNet b1.58 and\\nLLaMA LLM. We focus mainly on the calculation for matrix multiplication, since it contributes\\nthe most to the cost of LLMs. Figure 3 illustrates the composition of the energy cost. The majority\\nof BitNet b1.58 is INT8 addition calculation, while LLaMA LLM consists of both FP16 addition\\nand FP16 multiplication. According to the energy model in [Hor14, ZZL22], BitNet b1.58 saves\\n71.4 times arithmetic operations energy consumption for matrix multiplication on 7nm chips. We\\nfurther reported the end-to-end energy cost for models with 512 tokens. Our results show that as the\\nmodel size scales, BitNet b1.58 becomes increasingly more efficient in terms of energy consumption\\ncompared to the FP16 LLaMA LLM baseline. This is due to the fact that the percentage of nn.Linear\\ngrows with the model size, while the cost from other components is smaller for larger models.\\nThroughput\\nWe compare the throughput of BitNet b1.58 and LLaMA LLM with 70B parameters\\non two 80GB A100 cards, using pipeline parallelism [HCB+19] so that LLaMA LLM 70B could be\\nrun on the devices. We increased the batch size until the GPU memory limit was reached, with a\\nsequence length of 512. Table 3 shows that BitNet b1.58 70B can support up to 11 times the batch\\nsize of LLaMA LLM, resulting an 8.9 times higher throughput.\\n4https://github.com/EleutherAI/lm-evaluation-harness\\n4\\nBitNet b1.58\\nLLaMA\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n7nm Energy Cost (pJ)\\n71.4x\\nINT8 Add\\nFP16 Add\\nFP16 Mul\\n1.3B\\n3B\\n7B\\n13B\\n70B\\nModel Size\\n10\\n1\\n100\\n101\\nEnergy (J)\\n18.6x\\n21.7x\\n29.1x\\n32.9x\\n41.2x\\nBitNet b1.58\\nLLaMA\\nFigure 3: Energy consumption of BitNet b1.58 compared to LLaMA LLM at 7nm process nodes. On\\nthe left is the components of arithmetic operations energy. On the right is the end-to-end energy cost\\nacross different model sizes.\\nModels\\nTokens\\nWinogrande\\nPIQA\\nSciQ\\nLAMBADA\\nARC-easy\\nAvg.\\nStableLM-3B\\n2T\\n64.56\\n76.93\\n90.75\\n66.09\\n67.78\\n73.22\\nBitNet b1.58 3B\\n2T\\n66.37\\n78.40\\n91.20\\n67.63\\n68.12\\n74.34\\nTable 4: Comparison of BitNet b1.58 with StableLM-3B with 2T tokens.\\nBitNet b1.58 is enabling a new scaling law with respect to model performance and inference\\ncost. As a reference, we can have the following equivalence between different model sizes in 1.58-bit\\nand 16-bit based on the results in Figure 2 and 3.\\n• 13B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consump-\\ntion, than 3B FP16 LLM.\\n• 30B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consump-\\ntion, than 7B FP16 LLM.\\n• 70B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consump-\\ntion, than 13B FP16 LLM.\\nTraining with 2T Tokens\\nThe number of training tokens is a crucial factor for LLMs. To test\\nthe scalability of BitNet b1.58 in terms of tokens, we trained a BitNet b1.58 model with 2T to-\\nkens following the data recipe of StableLM-3B [TBMR], which is the state-of-the-art open-source\\n3B model. Both models were evaluated on a benchmark that consists of Winogrande [SBBC20],\\nPIQA [BZB+19], SciQ [WLG17], LAMBADA [PKL+16], and ARC-easy [YBS19]. We reported\\nthe zero-shot accuracy in Table 4. For tasks measured with accuracy and normalized accuracy, we\\ntake the average of the two. The results of StableLM 3b at 2T tokens are taken directly from its\\ntechnical report. Our findings shows that BitNet b1.58 achieves a superior performance on all end\\ntasks, indicating that 1.58-bit LLMs also have strong generalization capabilities.\\n4\\nDiscussion and Future Work\\n1-bit Mixture-of-Experts (MoE) LLMs\\nMixture-of-Experts (MoE) have proven to be a cost-effective approach for LLMs. While it signifi-\\ncantly reduces the computation FLOPs, the high memory consumption and inter-chip communication\\noverhead limit its deployment and application. These challenges can be addressed by 1.58-bit LLMs.\\nFirstly, the reduced memory footprint reduces the number of devices required to deploy MoE models.\\nMoreover, it significantly reduces the overhead of transferring activations across networks. Ultimately,\\nthere would be no overhead if the entire models could be placed on a single chip.\\n5\\nNative Support of Long Sequence in LLMs\\nIn the era of LLMs, the ability to handle long sequence has become a critical demand. One major\\nchallenge for long sequence inference is the memory consumption introduced by the KV caches.\\nBitNet b1.58 represents a significant step towards native support for long sequences, as it reduces the\\nactivations from 16 bits to 8 bits, allowing the context length to be doubled given the same resources.\\nThis can be further losslessly compressed to 4 bits or even lower for 1.58-bit LLMs, which we leave\\nas future work.\\nLLMs on Edge and Mobile\\nThe use of 1.58-bit LLMs has the potential to greatly improve the performance of language models\\non edge and mobile devices. These devices are often limited by their memory and computational\\npower, which can restrict the performance and the scale of LLMs. However, the reduced memory and\\nenergy consumption of 1.58-bit LLMs allows them to be deployed on these devices, enabling a wide\\nrange of applications that were previously not possible. This can greatly enhance the capabilities\\nof edge and mobile devices and enable new and exciting applications of LLMs. Moreover, 1.58-bit\\nLLMs are more friendly to CPU devices, which are the main processors used in edge and mobile\\ndevices. This means that BitNet b1.58 can be efficiently executed on these devices, further improving\\ntheir performance and capabilities.\\nNew Hardware for 1-bit LLMs\\nRecent work like Groq5 has demonstrated promising results and great potential for building specific\\nhardware (e.g., LPUs) for LLMs. Going one step further, we envision and call for actions to design\\nnew hardware and system specifically optimized for 1-bit LLMs, given the new computation paradigm\\nenabled in BitNet [WMD+23].\\nReferences\\n[BZB+19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA:\\nreasoning about physical commonsense in natural language. CoRR, abs/1911.11641,\\n2019.\\n[CCKS23] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. QuIP: 2-bit\\nquantization of large language models with guarantees. CoRR, abs/2307.13304, 2023.\\n[CLC+19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins,\\nand Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no\\nquestions. CoRR, abs/1905.10044, 2019.\\n[Com23] Together Computer. Redpajama: an open dataset for training large language models,\\n2023.\\n[FAHA23] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: accurate\\nquantization for generative pre-trained transformers. In The Eleventh International\\nConference on Learning Representations, 2023.\\n[HCB+19] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu Chen,\\nHyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe:\\nEfficient training of giant neural networks using pipeline parallelism. In Advances in\\nNeural Information Processing Systems, pages 103–112, 2019.\\n[Hor14] Mark Horowitz. 1.1 computing’s energy problem (and what we can do about it). In 2014\\nIEEE International Conference on Solid-State Circuits Conference, ISSCC 2014, Digest\\nof Technical Papers, San Francisco, CA, USA, February 9-13, 2014, pages 10–14, 2014.\\n[KLZ+23] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao\\nYu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for\\nlarge language model serving with pagedattention. In Proceedings of the ACM SIGOPS\\n29th Symposium on Operating Systems Principles, 2023.\\n5https://groq.com/\\n6\\n[LTT+23] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. AWQ:\\nactivation-aware weight quantization for LLM compression and acceleration. CoRR,\\nabs/2306.00978, 2023.\\n[MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of\\narmor conduct electricity? A new dataset for open book question answering. CoRR,\\nabs/1809.02789, 2018.\\n[MXBS16] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel\\nmixture models, 2016.\\n[PKL+16] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella\\nBernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The\\nLAMBADA dataset: Word prediction requiring a broad discourse context. In Proceed-\\nings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL\\n2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association\\nfor Computer Linguistics, 2016.\\n[RSR+19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning\\nwith a unified text-to-text transformer. CoRR, abs/1910.10683, 2019.\\n[SAL+24] Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.\\nRoformer: Enhanced transformer with rotary position embedding. Neurocomputing,\\n568:127063, 2024.\\n[SBBC20] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Wino-\\nGrande: an adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI\\nConference on Artificial Intelligence, pages 8732–8740, 2020.\\n[Sha20] Noam Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020.\\n[TBMR] Jonathan Tow, Marco Bellagente, Dakota Mahan, and Carlos Riquelme. Stablelm 3b\\n4e1t.\\n[TCS+24] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De\\nSa. Quip#: Even better LLM quantization with hadamard incoherence and lattice\\ncodebooks. CoRR, abs/2402.04396, 2024.\\n[TLI+23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\\nTimothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien\\nRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: open and\\nefficient foundation language models. CoRR, abs/2302.13971, 2023.\\n[TMS+23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine\\nBabaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan\\nBikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David\\nEsiobu, Jude Fernandes, Jeremy Fu, and et al. Llama 2: open foundation and fine-tuned\\nchat models. CoRR, abs/2307.09288, 2023.\\n[WLG17] Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice\\nscience questions. In Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin, editors,\\nProceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017,\\nCopenhagen, Denmark, September 7, 2017, pages 94–106. Association for Computa-\\ntional Linguistics, 2017.\\n[WMC+23] Lei Wang, Lingxiao Ma, Shijie Cao, Ningxin Zheng, Quanlu Zhang, Jilong Xue, Ziming\\nMiao, Ting Cao, , and Yuqing Yang. Ladder: Efficient tensor compilation on customized\\ndata format. In OSDI, 2023.\\n[WMD+23] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma,\\nFan Yang, Ruiping Wang, Yi Wu, and Furu Wei. Bitnet: Scaling 1-bit transformers for\\nlarge language models. CoRR, abs/2310.11453, 2023.\\n7\\n[XLS+23] Guangxuan Xiao, Ji Lin, Mickaël Seznec, Hao Wu, Julien Demouth, and Song Han.\\nSmoothQuant: accurate and efficient post-training quantization for large language\\nmodels. In International Conference on Machine Learning, ICML 2023, 23-29 July\\n2023, Honolulu, Hawaii, USA, 2023.\\n[YBS19] Vikas Yadav, Steven Bethard, and Mihai Surdeanu. Quick and (not so) dirty: Unsuper-\\nvised selection of justification sentences for multi-hop question answering. In Kentaro\\nInui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, EMNLP-IJCNLP, 2019.\\n[ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag:\\ncan a machine really finish your sentence? In Proceedings of the 57th Conference of\\nthe Association for Computational Linguistics, pages 4791–4800, 2019.\\n[ZS19] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In Hanna M.\\nWallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox,\\nand Roman Garnett, editors, Advances in Neural Information Processing Systems, pages\\n12360–12371, 2019.\\n[ZZL22] Yichi Zhang, Zhiru Zhang, and Lukasz Lew. PokeBNN: A binary pursuit of lightweight\\naccuracy. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\\n12465–12475. IEEE, 2022.\\n8\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b536c57-29ed-40f6-acc9-087cc1b52922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, chunk_size, chunk_overlap):\n",
    "    \"\"\"\n",
    "    Splits a text into chunks with specified size and overlap.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to be split.\n",
    "        chunk_size (int): The desired chunk size.\n",
    "        chunk_overlap (int): The number of characters to overlap between chunks.\n",
    "        \n",
    "    Returns:\n",
    "        list of str: A list containing the text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    end = chunk_size\n",
    "    while start < len(text):\n",
    "        # Extract the chunk\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        # Update start and end taking into account the overlap\n",
    "        start += chunk_size - chunk_overlap\n",
    "        end = start + chunk_size\n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "pdf_text = pdf_text  # This should be your extracted PDF text\n",
    "chunks = split_text_into_chunks(pdf_text, chunk_size, chunk_overlap)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61cf5b33-338a-426f-bbb3-0e6bf21bf5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Era of 1-bit LLMs:\\nAll Large Language Models are in 1.58 Bits\\nShuming Ma∗ Hongyu Wang∗ Lingxiao Ma\\nLei Wang\\nWenhui Wang\\nShaohan Huang\\nLi Dong\\nRuiping Wang\\nJilong Xue\\nFuru Wei⋄\\nhttps://aka.ms/GeneralAI\\nAbstract\\nRecent research, such as BitNet [WMD+23], is paving the way for a new era of 1-\\nbit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,\\nnamely BitNet b1.58, in which every single parameter (or weight) of the LLM is\\nternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer\\nLLM with the same model size and training tokens in terms of both perplexity\\nand end-task performance, while being significantly more cost-effective in terms\\nof latency, memory, throughput, and energy consumption. More profoundly, the\\n1.58-bit LLM defines a new scaling law and recipe for training new generations of\\nLLMs that are both high-performance and cost-effective. Furthermore, it enables\\na new computation paradigm and opens the door for designing speci',\n",
       " 'scaling law and recipe for training new generations of\\nLLMs that are both high-performance and cost-effective. Furthermore, it enables\\na new computation paradigm and opens the door for designing specific hardware\\noptimized for 1-bit LLMs.\\n0.2961 -0.0495\\n0.0413\\n...\\n…\\n-0.4765\\n0.2812\\n0.2403\\n-0.1808 0.1304\\n-0.4809\\n…\\n…\\n-0.1771\\n-0.1741 -0.3853\\nTransformer LLMs\\n16-bit Float (FP16/BF16)\\nCost\\nPerformance\\n1\\n-1\\n0\\n…\\n…\\n1\\n-1\\n-1\\n-1\\n1\\n-1\\n…\\n…\\n0\\n0\\n-1\\nBitNet b1.58 (This Work)\\n{-1, 0, 1}\\nW=\\nPareto Improvement\\nW=\\n0.0413\\n0.3397\\n0.2812 0.2403\\n-0.1808 0.1304\\n-0.4809 0.3244\\n0.4322 -0.1771\\n-0.1741 -0.3853\\n0.2961\\n-0.0495 -0.0924 -0.4765\\n𝒙𝟎\\n𝒙𝟏\\n𝒙𝟐\\n𝒙𝟑\\n𝟎. 𝟐𝟗𝟔𝟏𝒙𝟎 − 𝟎. 𝟎𝟒𝟗𝟓𝒙𝟏 − 𝟎. 𝟎𝟗𝟐𝟒𝒙𝟐 − 𝟎. 𝟒𝟕𝟔𝟓𝒙𝟑\\n…\\n1\\n-1\\n0\\n1\\n-1\\n1\\n-1\\n-1\\n-1\\n0\\n-1\\n1\\n1\\n-1\\n1\\n0\\n𝒙𝟎 − 𝒙𝟏 − 𝒙𝟐 + 𝒙𝟑\\n…\\n𝒙𝟎\\n𝒙𝟏\\n𝒙𝟐\\n𝒙𝟑\\n1(.58)-bit\\nFP16\\nModel W\\nInput X\\nOutput Y\\nY = f(W, X)\\nGPU\\nNew \\nHardware \\nFigure 1: 1-bit LLMs (e.g., BitNet b1.58) provide a Pareto solution to reduce inference cost (latency,\\nthroughput, and energy) of LLMs while maintaining model performance. The new',\n",
       " ')\\nGPU\\nNew \\nHardware \\nFigure 1: 1-bit LLMs (e.g., BitNet b1.58) provide a Pareto solution to reduce inference cost (latency,\\nthroughput, and energy) of LLMs while maintaining model performance. The new computation\\nparadigm of BitNet b1.58 calls for actions to design new hardware optimized for 1-bit LLMs.\\n∗ Equal contribution. ⋄ Corresponding author. S. Ma, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, J. Xue,\\nF. Wei are with Microsoft Research. H. Wang and R. Wang are with University of Chinese Academy of Sciences.\\narXiv:2402.17764v1  [cs.CL]  27 Feb 2024\\n1\\nThe Era of 1-bit LLMs\\nIn recent years, the field of AI has seen a rapid growth in the size and capabilities of Large Language\\nModels (LLMs). These models have demonstrated remarkable performance in a wide range of natural\\nlanguage processing tasks, but their increasing size has posed challenges for deployment and raised\\nconcerns about their environmental and economic impact due to high energy consumption. One\\napproach to address these ',\n",
       " 'ng tasks, but their increasing size has posed challenges for deployment and raised\\nconcerns about their environmental and economic impact due to high energy consumption. One\\napproach to address these challenges is to use post-training quantization to create low-bit models\\nfor inference [XLS+23, FAHA23, CCKS23, TCS+24]. This technique reduces the precision of\\nweights and activations, significantly reducing the memory and computational requirements of LLMs.\\nThe trend has been to move from 16 bits to lower bits, such as 4-bit variants [FAHA23, LTT+23].\\nHowever, post-training quantization is sub-optimal, even though it is widely used in industry LLMs.\\nRecent work on 1-bit model architectures, such as BitNet [WMD+23], presents a promising direction\\nfor reducing the cost of LLMs while maintaining their performance. Vanilla LLMs are in 16-bit\\nfloating values (i.e., FP16 or BF16), and the bulk of any LLMs is matrix multiplication. Therefore,\\nthe major computation cost comes from the floating-p',\n",
       " 'g their performance. Vanilla LLMs are in 16-bit\\nfloating values (i.e., FP16 or BF16), and the bulk of any LLMs is matrix multiplication. Therefore,\\nthe major computation cost comes from the floating-point addition and multiplication operations. In\\ncontrast, the matrix multiplication of BitNet only involves integer addition, which saves orders of\\nenergy cost for LLMs. As the fundamental limit to compute performance in many chips is power, the\\nenergy savings can also be translated into faster computation.\\nIn addition to computation, the process of transferring model parameters from DRAM to the memory\\nof an on-chip accelerator (e.g., SRAM) can be expensive during inference. There have been attempts\\nto enlarge SRAM to improve throughput, but this introduces significantly higher costs than DRAM.\\nCompared to full-precision models, 1-bit LLMs have a much lower memory footprint from both a\\ncapacity and bandwidth standpoint. This can significantly reduce the cost and time of loading weights\\nfro',\n",
       " '.\\nCompared to full-precision models, 1-bit LLMs have a much lower memory footprint from both a\\ncapacity and bandwidth standpoint. This can significantly reduce the cost and time of loading weights\\nfrom DRAM, leading to faster and more efficient inference.\\nIn this work, we introduce a significant 1-bit LLM variant called BitNet b1.58, where every parameter\\nis ternary, taking on values of {-1, 0, 1}. We have added an additional value of 0 to the original 1-bit\\nBitNet, resulting in 1.58 bits in the binary system. BitNet b1.58 retains all the benefits of the original\\n1-bit BitNet, including its new computation paradigm, which requires almost no multiplication\\noperations for matrix multiplication and can be highly optimized. Additionally, it has the same energy\\nconsumption as the original 1-bit BitNet and is much more efficient in terms of memory consumption,\\nthroughput and latency compared to FP16 LLM baselines. Furthermore, BitNet b1.58 offers two\\nadditional advantages. Firstly, its model',\n",
       " ' BitNet and is much more efficient in terms of memory consumption,\\nthroughput and latency compared to FP16 LLM baselines. Furthermore, BitNet b1.58 offers two\\nadditional advantages. Firstly, its modeling capability is stronger due to its explicit support for feature\\nfiltering, made possible by the inclusion of 0 in the model weights, which can significantly improve\\nthe performance of 1-bit LLMs. Secondly, our experiments show that BitNet b1.58 can match full\\nprecision (i.e., FP16) baselines in terms of both perplexity and end-task performance, starting from a\\n3B size, when using the same configuration (e.g., model size, training tokens, etc.).\\n2\\nBitNet b1.58\\nBitNet b1.58 is based on the BitNet architecture, which is a Transformer that replaces nn.Linear with\\nBitLinear. It is trained from scratch, with 1.58-bit weights and 8-bit activations. Compared to the\\noriginal BitNet, it introduces some modifications that we summarize below.\\nQuantization Function.\\nTo constrain the weights to -1, 0',\n",
       " 'cratch, with 1.58-bit weights and 8-bit activations. Compared to the\\noriginal BitNet, it introduces some modifications that we summarize below.\\nQuantization Function.\\nTo constrain the weights to -1, 0, or +1, we adopt an absmean quantization\\nfunction. It first scales the weight matrix by its average absolute value, and then round each value to\\nthe nearest integer among {-1, 0, +1}:\\nf\\nW = RoundClip( W\\nγ + ϵ, −1, 1),\\n(1)\\nRoundClip(x, a, b) = max(a, min(b, round(x))),\\n(2)\\nγ =\\n1\\nnm\\nX\\nij\\n|Wij|.\\n(3)\\nThe quantization function for activations follows the same implementation in BitNet, except that\\nwe do not scale the activations before the non-linear functions to the range [0, Qb]. Instead, the\\n2\\nModels\\nSize\\nMemory (GB)↓\\nLatency (ms)↓\\nPPL↓\\nLLaMA LLM\\n700M\\n2.08 (1.00x)\\n1.18 (1.00x)\\n12.33\\nBitNet b1.58\\n700M\\n0.80 (2.60x)\\n0.96 (1.23x)\\n12.87\\nLLaMA LLM\\n1.3B\\n3.34 (1.00x)\\n1.62 (1.00x)\\n11.25\\nBitNet b1.58\\n1.3B\\n1.14 (2.93x)\\n0.97 (1.67x)\\n11.29\\nLLaMA LLM\\n3B\\n7.89 (1.00x)\\n5.07 (1.00x)\\n10.04\\nBitNet b1.58\\n3B\\n2.22',\n",
       " '\\n700M\\n0.80 (2.60x)\\n0.96 (1.23x)\\n12.87\\nLLaMA LLM\\n1.3B\\n3.34 (1.00x)\\n1.62 (1.00x)\\n11.25\\nBitNet b1.58\\n1.3B\\n1.14 (2.93x)\\n0.97 (1.67x)\\n11.29\\nLLaMA LLM\\n3B\\n7.89 (1.00x)\\n5.07 (1.00x)\\n10.04\\nBitNet b1.58\\n3B\\n2.22 (3.55x)\\n1.87 (2.71x)\\n9.91\\nBitNet b1.58\\n3.9B\\n2.38 (3.32x)\\n2.11 (2.40x)\\n9.62\\nTable 1: Perplexity as well as the cost of BitNet b1.58 and LLaMA LLM.\\nModels\\nSize\\nARCe\\nARCc\\nHS\\nBQ\\nOQ\\nPQ\\nWGe\\nAvg.\\nLLaMA LLM\\n700M\\n54.7\\n23.0\\n37.0\\n60.0\\n20.2\\n68.9\\n54.8\\n45.5\\nBitNet b1.58\\n700M\\n51.8\\n21.4\\n35.1\\n58.2\\n20.0\\n68.1\\n55.2\\n44.3\\nLLaMA LLM\\n1.3B\\n56.9\\n23.5\\n38.5\\n59.1\\n21.6\\n70.0\\n53.9\\n46.2\\nBitNet b1.58\\n1.3B\\n54.9\\n24.2\\n37.7\\n56.7\\n19.6\\n68.8\\n55.8\\n45.4\\nLLaMA LLM\\n3B\\n62.1\\n25.6\\n43.3\\n61.8\\n24.6\\n72.1\\n58.2\\n49.7\\nBitNet b1.58\\n3B\\n61.4\\n28.3\\n42.9\\n61.5\\n26.6\\n71.5\\n59.3\\n50.2\\nBitNet b1.58\\n3.9B\\n64.2\\n28.7\\n44.2\\n63.5\\n24.2\\n73.2\\n60.5\\n51.2\\nTable 2: Zero-shot accuracy of BitNet b1.58 and LLaMA LLM on the end tasks.\\nactivations are all scaled to [−Qb, Qb] per token to get rid of the zero-point quantization. This is\\nmore convenient and simple for both impl',\n",
       " 't accuracy of BitNet b1.58 and LLaMA LLM on the end tasks.\\nactivations are all scaled to [−Qb, Qb] per token to get rid of the zero-point quantization. This is\\nmore convenient and simple for both implementation and system-level optimization, while introduces\\nnegligible effects to the performance in our experiments.\\nLLaMA-alike Components.\\nThe architecture of LLaMA [TLI+23, TMS+23] has been the de-\\nfacto backbone for open-source LLMs.\\nTo embrace the open-source community, our design\\nof BitNet b1.58 adopts the LLaMA-alike components. Specifically, it uses RMSNorm [ZS19],\\nSwiGLU [Sha20], rotary embedding [SAL+24], and removes all biases. In this way, BitNet b1.58\\ncan be integrated into the popular open-source software (e.g., Huggingface, vLLM [KLZ+23], and\\nllama.cpp2) with minimal efforts.\\n3\\nResults\\nWe compared BitNet b1.58 to our reproduced FP16 LLaMA LLM in various sizes. To ensure a fair\\ncomparison, we pre-trained the models on the RedPajama dataset [Com23] for 100 billion tokens.\\nWe e',\n",
       " 'Results\\nWe compared BitNet b1.58 to our reproduced FP16 LLaMA LLM in various sizes. To ensure a fair\\ncomparison, we pre-trained the models on the RedPajama dataset [Com23] for 100 billion tokens.\\nWe evaluated the zero-shot performance on a range of language tasks, including ARC-Easy [YBS19],\\nARC-Challenge [YBS19], Hellaswag [ZHB+19], Winogrande [SBBC20], PIQA [BZB+19], Open-\\nbookQA [MCKS18], and BoolQ [CLC+19]. We also reported the validation perplexity on the\\nWikiText2 [MXBS16] and C4 [RSR+19] datasets.\\nWe compared the runtime GPU memory and latency of both LLaMA LLM and BitNet b1.58. The\\nresults were measured using the FasterTransformer3 codebase, which is well-optimized for LLM\\ninference latency on GPU devices. The 2-bit kernel from Ladder [WMC+23] is also integrated for\\nBitNet b1.58. We reported the time per output token, as it is the major cost for inference.\\nTable 1 summarizes the perplexity and the cost for BitNet b1.58 and LLaMA LLM. It shows that\\nBitNet b1.58 starts to match f',\n",
       " 'e reported the time per output token, as it is the major cost for inference.\\nTable 1 summarizes the perplexity and the cost for BitNet b1.58 and LLaMA LLM. It shows that\\nBitNet b1.58 starts to match full precision LLaMA LLM at 3B model size in terms of perplexity,\\nwhile being 2.71 times faster and using 3.55 times less GPU memory. In particular, BitNet b1.58 with\\na 3.9B model size is 2.4 times faster, consumes 3.32 times less memory, but performs significantly\\nbetter than LLaMA LLM 3B.\\n2https://github.com/ggerganov/llama.cpp\\n3https://github.com/NVIDIA/FasterTransformer\\n3\\n1.3B\\n3B\\n7B\\n13B\\n70B\\nModel Size\\n100\\n101\\n102\\nLatency (ms)\\n1.67x\\n2.71x\\n2.90x\\n3.68x\\n4.10x\\nBitNet b1.58\\nLLaMA\\n1.3B\\n3B\\n7B\\n13B\\n70B\\nModel Size\\n100\\n101\\n102\\nMemory (GB)\\n2.93x\\n3.55x\\n4.40x\\n5.12x\\n7.16x\\nBitNet b1.58\\nLLaMA\\nFigure 2: Decoding latency (Left) and memory consumption (Right) of BitNet b1.58 varying the\\nmodel size.\\nModels\\nSize\\nMax Batch Size\\nThroughput (tokens/s)\\nLLaMA LLM\\n70B\\n16 (1.0x)\\n333 (1.0x)\\nBitNet b1.58\\n70B\\n176 (11.0',\n",
       " 'ing latency (Left) and memory consumption (Right) of BitNet b1.58 varying the\\nmodel size.\\nModels\\nSize\\nMax Batch Size\\nThroughput (tokens/s)\\nLLaMA LLM\\n70B\\n16 (1.0x)\\n333 (1.0x)\\nBitNet b1.58\\n70B\\n176 (11.0x)\\n2977 (8.9x)\\nTable 3: Comparison of the throughput between BitNet b1.58 70B and LLaMA LLM 70B.\\nTable 2 reports the detailed results of the zero-shot accuracy on the end tasks. We followed the pipeline\\nfrom lm-evaluation-harness4 to perform the evaluation. The results show that the performance gap\\nbetween BitNet b1.58 and LLaMA LLM narrows as the model size increases. More importantly,\\nBitNet b1.58 can match the performance of the full precision baseline starting from a 3B size. Similar\\nto the observation of the perplexity, the end-task results reveal that BitNet b1.58 3.9B outperforms\\nLLaMA LLM 3B with lower memory and latency cost. This demonstrates that BitNet b1.58 is a\\nPareto improvement over the state-of-the-art LLM models.\\nMemory and Latency\\nWe further scaled up the model size to 7',\n",
       " 'LLM 3B with lower memory and latency cost. This demonstrates that BitNet b1.58 is a\\nPareto improvement over the state-of-the-art LLM models.\\nMemory and Latency\\nWe further scaled up the model size to 7B, 13B, and 70B and evaluated the\\ncost. Figure 2 illustrates the trends of latency and memory, showing that the speed-up increases as the\\nmodel size scales. In particular, BitNet b1.58 70B is 4.1 times faster than the LLaMA LLM baseline.\\nThis is because the time cost for nn.Linear grows with the model size. The memory consumption\\nfollows a similar trend, as the embedding remains full precision and its memory proportion is smaller\\nfor larger models. Both latency and memory were measured with a 2-bit kernel, so there is still room\\nfor optimization to further reduce the cost.\\nEnergy\\nWe also estimate the arithmetic operations energy consumption of both BitNet b1.58 and\\nLLaMA LLM. We focus mainly on the calculation for matrix multiplication, since it contributes\\nthe most to the cost of LLMs. Fi',\n",
       " 'ate the arithmetic operations energy consumption of both BitNet b1.58 and\\nLLaMA LLM. We focus mainly on the calculation for matrix multiplication, since it contributes\\nthe most to the cost of LLMs. Figure 3 illustrates the composition of the energy cost. The majority\\nof BitNet b1.58 is INT8 addition calculation, while LLaMA LLM consists of both FP16 addition\\nand FP16 multiplication. According to the energy model in [Hor14, ZZL22], BitNet b1.58 saves\\n71.4 times arithmetic operations energy consumption for matrix multiplication on 7nm chips. We\\nfurther reported the end-to-end energy cost for models with 512 tokens. Our results show that as the\\nmodel size scales, BitNet b1.58 becomes increasingly more efficient in terms of energy consumption\\ncompared to the FP16 LLaMA LLM baseline. This is due to the fact that the percentage of nn.Linear\\ngrows with the model size, while the cost from other components is smaller for larger models.\\nThroughput\\nWe compare the throughput of BitNet b1.58 and LL',\n",
       " 'e to the fact that the percentage of nn.Linear\\ngrows with the model size, while the cost from other components is smaller for larger models.\\nThroughput\\nWe compare the throughput of BitNet b1.58 and LLaMA LLM with 70B parameters\\non two 80GB A100 cards, using pipeline parallelism [HCB+19] so that LLaMA LLM 70B could be\\nrun on the devices. We increased the batch size until the GPU memory limit was reached, with a\\nsequence length of 512. Table 3 shows that BitNet b1.58 70B can support up to 11 times the batch\\nsize of LLaMA LLM, resulting an 8.9 times higher throughput.\\n4https://github.com/EleutherAI/lm-evaluation-harness\\n4\\nBitNet b1.58\\nLLaMA\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n7nm Energy Cost (pJ)\\n71.4x\\nINT8 Add\\nFP16 Add\\nFP16 Mul\\n1.3B\\n3B\\n7B\\n13B\\n70B\\nModel Size\\n10\\n1\\n100\\n101\\nEnergy (J)\\n18.6x\\n21.7x\\n29.1x\\n32.9x\\n41.2x\\nBitNet b1.58\\nLLaMA\\nFigure 3: Energy consumption of BitNet b1.58 compared to LLaMA LLM at 7nm process nodes. On\\nthe left is the components of arithmetic operations energy. On the right is the e',\n",
       " 'x\\n41.2x\\nBitNet b1.58\\nLLaMA\\nFigure 3: Energy consumption of BitNet b1.58 compared to LLaMA LLM at 7nm process nodes. On\\nthe left is the components of arithmetic operations energy. On the right is the end-to-end energy cost\\nacross different model sizes.\\nModels\\nTokens\\nWinogrande\\nPIQA\\nSciQ\\nLAMBADA\\nARC-easy\\nAvg.\\nStableLM-3B\\n2T\\n64.56\\n76.93\\n90.75\\n66.09\\n67.78\\n73.22\\nBitNet b1.58 3B\\n2T\\n66.37\\n78.40\\n91.20\\n67.63\\n68.12\\n74.34\\nTable 4: Comparison of BitNet b1.58 with StableLM-3B with 2T tokens.\\nBitNet b1.58 is enabling a new scaling law with respect to model performance and inference\\ncost. As a reference, we can have the following equivalence between different model sizes in 1.58-bit\\nand 16-bit based on the results in Figure 2 and 3.\\n• 13B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consump-\\ntion, than 3B FP16 LLM.\\n• 30B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consump-\\ntion, than 7B FP16 LLM.\\n• 70B BitNet b1.58 is more efficient, in',\n",
       " 'and energy consump-\\ntion, than 3B FP16 LLM.\\n• 30B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consump-\\ntion, than 7B FP16 LLM.\\n• 70B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consump-\\ntion, than 13B FP16 LLM.\\nTraining with 2T Tokens\\nThe number of training tokens is a crucial factor for LLMs. To test\\nthe scalability of BitNet b1.58 in terms of tokens, we trained a BitNet b1.58 model with 2T to-\\nkens following the data recipe of StableLM-3B [TBMR], which is the state-of-the-art open-source\\n3B model. Both models were evaluated on a benchmark that consists of Winogrande [SBBC20],\\nPIQA [BZB+19], SciQ [WLG17], LAMBADA [PKL+16], and ARC-easy [YBS19]. We reported\\nthe zero-shot accuracy in Table 4. For tasks measured with accuracy and normalized accuracy, we\\ntake the average of the two. The results of StableLM 3b at 2T tokens are taken directly from its\\ntechnical report. Our findings shows that BitNet b1.58 achieves a superior ',\n",
       " 'and normalized accuracy, we\\ntake the average of the two. The results of StableLM 3b at 2T tokens are taken directly from its\\ntechnical report. Our findings shows that BitNet b1.58 achieves a superior performance on all end\\ntasks, indicating that 1.58-bit LLMs also have strong generalization capabilities.\\n4\\nDiscussion and Future Work\\n1-bit Mixture-of-Experts (MoE) LLMs\\nMixture-of-Experts (MoE) have proven to be a cost-effective approach for LLMs. While it signifi-\\ncantly reduces the computation FLOPs, the high memory consumption and inter-chip communication\\noverhead limit its deployment and application. These challenges can be addressed by 1.58-bit LLMs.\\nFirstly, the reduced memory footprint reduces the number of devices required to deploy MoE models.\\nMoreover, it significantly reduces the overhead of transferring activations across networks. Ultimately,\\nthere would be no overhead if the entire models could be placed on a single chip.\\n5\\nNative Support of Long Sequence in LLMs\\nIn the era',\n",
       " 'overhead of transferring activations across networks. Ultimately,\\nthere would be no overhead if the entire models could be placed on a single chip.\\n5\\nNative Support of Long Sequence in LLMs\\nIn the era of LLMs, the ability to handle long sequence has become a critical demand. One major\\nchallenge for long sequence inference is the memory consumption introduced by the KV caches.\\nBitNet b1.58 represents a significant step towards native support for long sequences, as it reduces the\\nactivations from 16 bits to 8 bits, allowing the context length to be doubled given the same resources.\\nThis can be further losslessly compressed to 4 bits or even lower for 1.58-bit LLMs, which we leave\\nas future work.\\nLLMs on Edge and Mobile\\nThe use of 1.58-bit LLMs has the potential to greatly improve the performance of language models\\non edge and mobile devices. These devices are often limited by their memory and computational\\npower, which can restrict the performance and the scale of LLMs. However, the redu',\n",
       " 'ance of language models\\non edge and mobile devices. These devices are often limited by their memory and computational\\npower, which can restrict the performance and the scale of LLMs. However, the reduced memory and\\nenergy consumption of 1.58-bit LLMs allows them to be deployed on these devices, enabling a wide\\nrange of applications that were previously not possible. This can greatly enhance the capabilities\\nof edge and mobile devices and enable new and exciting applications of LLMs. Moreover, 1.58-bit\\nLLMs are more friendly to CPU devices, which are the main processors used in edge and mobile\\ndevices. This means that BitNet b1.58 can be efficiently executed on these devices, further improving\\ntheir performance and capabilities.\\nNew Hardware for 1-bit LLMs\\nRecent work like Groq5 has demonstrated promising results and great potential for building specific\\nhardware (e.g., LPUs) for LLMs. Going one step further, we envision and call for actions to design\\nnew hardware and system specificall',\n",
       " 'rated promising results and great potential for building specific\\nhardware (e.g., LPUs) for LLMs. Going one step further, we envision and call for actions to design\\nnew hardware and system specifically optimized for 1-bit LLMs, given the new computation paradigm\\nenabled in BitNet [WMD+23].\\nReferences\\n[BZB+19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA:\\nreasoning about physical commonsense in natural language. CoRR, abs/1911.11641,\\n2019.\\n[CCKS23] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. QuIP: 2-bit\\nquantization of large language models with guarantees. CoRR, abs/2307.13304, 2023.\\n[CLC+19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins,\\nand Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no\\nquestions. CoRR, abs/1905.10044, 2019.\\n[Com23] Together Computer. Redpajama: an open dataset for training large language models,\\n2023.\\n[FAHA23] Elias Frantar, Saleh Ashkboos, To',\n",
       " 'ulty of natural yes/no\\nquestions. CoRR, abs/1905.10044, 2019.\\n[Com23] Together Computer. Redpajama: an open dataset for training large language models,\\n2023.\\n[FAHA23] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: accurate\\nquantization for generative pre-trained transformers. In The Eleventh International\\nConference on Learning Representations, 2023.\\n[HCB+19] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu Chen,\\nHyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe:\\nEfficient training of giant neural networks using pipeline parallelism. In Advances in\\nNeural Information Processing Systems, pages 103–112, 2019.\\n[Hor14] Mark Horowitz. 1.1 computing’s energy problem (and what we can do about it). In 2014\\nIEEE International Conference on Solid-State Circuits Conference, ISSCC 2014, Digest\\nof Technical Papers, San Francisco, CA, USA, February 9-13, 2014, pages 10–14, 2014.\\n[KLZ+23] Woosuk Kwon, Zhuohan Li, Siyuan Zh',\n",
       " 'ional Conference on Solid-State Circuits Conference, ISSCC 2014, Digest\\nof Technical Papers, San Francisco, CA, USA, February 9-13, 2014, pages 10–14, 2014.\\n[KLZ+23] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao\\nYu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for\\nlarge language model serving with pagedattention. In Proceedings of the ACM SIGOPS\\n29th Symposium on Operating Systems Principles, 2023.\\n5https://groq.com/\\n6\\n[LTT+23] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. AWQ:\\nactivation-aware weight quantization for LLM compression and acceleration. CoRR,\\nabs/2306.00978, 2023.\\n[MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of\\narmor conduct electricity? A new dataset for open book question answering. CoRR,\\nabs/1809.02789, 2018.\\n[MXBS16] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel\\nmixture models, 2016.\\n[PKL+16] Denis Paper',\n",
       " 'et for open book question answering. CoRR,\\nabs/1809.02789, 2018.\\n[MXBS16] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel\\nmixture models, 2016.\\n[PKL+16] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella\\nBernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The\\nLAMBADA dataset: Word prediction requiring a broad discourse context. In Proceed-\\nings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL\\n2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association\\nfor Computer Linguistics, 2016.\\n[RSR+19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning\\nwith a unified text-to-text transformer. CoRR, abs/1910.10683, 2019.\\n[SAL+24] Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.\\nRoformer: Enhanced transformer wi',\n",
       " 'earning\\nwith a unified text-to-text transformer. CoRR, abs/1910.10683, 2019.\\n[SAL+24] Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.\\nRoformer: Enhanced transformer with rotary position embedding. Neurocomputing,\\n568:127063, 2024.\\n[SBBC20] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Wino-\\nGrande: an adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI\\nConference on Artificial Intelligence, pages 8732–8740, 2020.\\n[Sha20] Noam Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020.\\n[TBMR] Jonathan Tow, Marco Bellagente, Dakota Mahan, and Carlos Riquelme. Stablelm 3b\\n4e1t.\\n[TCS+24] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De\\nSa. Quip#: Even better LLM quantization with hadamard incoherence and lattice\\ncodebooks. CoRR, abs/2402.04396, 2024.\\n[TLI+23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\\nTimothée Lacroix, Baptiste Roz',\n",
       " 'with hadamard incoherence and lattice\\ncodebooks. CoRR, abs/2402.04396, 2024.\\n[TLI+23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\\nTimothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien\\nRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: open and\\nefficient foundation language models. CoRR, abs/2302.13971, 2023.\\n[TMS+23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine\\nBabaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan\\nBikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David\\nEsiobu, Jude Fernandes, Jeremy Fu, and et al. Llama 2: open foundation and fine-tuned\\nchat models. CoRR, abs/2307.09288, 2023.\\n[WLG17] Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice\\nscience questions. In Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin, editors,\\nProceedings of the 3rd Workshop on Noisy User-gene',\n",
       " ' Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice\\nscience questions. In Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin, editors,\\nProceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017,\\nCopenhagen, Denmark, September 7, 2017, pages 94–106. Association for Computa-\\ntional Linguistics, 2017.\\n[WMC+23] Lei Wang, Lingxiao Ma, Shijie Cao, Ningxin Zheng, Quanlu Zhang, Jilong Xue, Ziming\\nMiao, Ting Cao, , and Yuqing Yang. Ladder: Efficient tensor compilation on customized\\ndata format. In OSDI, 2023.\\n[WMD+23] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma,\\nFan Yang, Ruiping Wang, Yi Wu, and Furu Wei. Bitnet: Scaling 1-bit transformers for\\nlarge language models. CoRR, abs/2310.11453, 2023.\\n7\\n[XLS+23] Guangxuan Xiao, Ji Lin, Mickaël Seznec, Hao Wu, Julien Demouth, and Song Han.\\nSmoothQuant: accurate and efficient post-training quantization for large language\\nmodels. In International Conference on Machine Learning, ICML 2023, 23',\n",
       " 'l Seznec, Hao Wu, Julien Demouth, and Song Han.\\nSmoothQuant: accurate and efficient post-training quantization for large language\\nmodels. In International Conference on Machine Learning, ICML 2023, 23-29 July\\n2023, Honolulu, Hawaii, USA, 2023.\\n[YBS19] Vikas Yadav, Steven Bethard, and Mihai Surdeanu. Quick and (not so) dirty: Unsuper-\\nvised selection of justification sentences for multi-hop question answering. In Kentaro\\nInui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, EMNLP-IJCNLP, 2019.\\n[ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag:\\ncan a machine really finish your sentence? In Proceedings of the 57th Conference of\\nthe Association for Computational Linguistics, pages 4791–4800, 2019.\\n[ZS19] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In Hanna M.\\nWallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox,\\nand Roman Garnett, editors, Advances in Neural Information Processing Systems, pages\\n1',\n",
       " 'layer normalization. In Hanna M.\\nWallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox,\\nand Roman Garnett, editors, Advances in Neural Information Processing Systems, pages\\n12360–12371, 2019.\\n[ZZL22] Yichi Zhang, Zhiru Zhang, and Lukasz Lew. PokeBNN: A binary pursuit of lightweight\\naccuracy. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\\n12465–12475. IEEE, 2022.\\n8\\n']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "efe476d1-9388-4549-842b-feb63d37dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Assuming `from_texts` exists and works directly with a list of strings\n",
    "vectorstore = Chroma.from_texts(\n",
    "    texts=[chunk for chunk in chunks],  # Directly use the list of text chunks\n",
    "    collection_name=\"ollama_embeds\",\n",
    "    embedding=embeddings.ollama.OllamaEmbeddings(model='nomic-embed-text'),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9428fcd3-4427-45ef-9733-b62c574cf346",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43623175-c0a2-45fb-a87a-5c816d846439",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "            groq_api_key=groq_api_key,\n",
    "            model_name='mixtral-8x7b-32768'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f339c40-5a22-4ba6-9dbc-121fbfc143c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5e9ad7c-cf39-47ff-821d-e6ee1e18f489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document is about the advantages and applications of low-bit language models\n",
      "(LLMs), specifically 1.58-bit LLMs, in terms of memory usage, energy\n",
      "consumption, and computational power. It also discusses the potential of 1-bit\n",
      "Mixture-of-Experts (MoE) LLMs and the development of new hardware for LLMs. The\n",
      "document also mentions several research papers and articles related to LLMs.\n"
     ]
    }
   ],
   "source": [
    "# Test the architecture with a simple hard coded question\n",
    "response = rag_chain.invoke(\"What is this document about\")\n",
    "print(textwrap.fill(response, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c17797a4-acf2-4683-8af5-badb2c99d18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the questions dynamic using a chat interface. Let's use gradio for this.\n",
    "def process_question(user_question):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Directly using the user's question as input for rag_chain.invoke\n",
    "    response = rag_chain.invoke(user_question)\n",
    "\n",
    "    # Measure the response time\n",
    "    end_time = time.time()\n",
    "    response_time = f\"Response time: {end_time - start_time:.2f} seconds.\"\n",
    "\n",
    "    # Combine the response and the response time into a single string\n",
    "    full_response = f\"{response}\\n\\n{response_time}\"\n",
    "\n",
    "    return full_response\n",
    "\n",
    "# Setup the Gradio interface\n",
    "iface = gr.Interface(fn=process_question,\n",
    "                     inputs=gr.Textbox(lines=2, placeholder=\"Type your question here...\"),\n",
    "                     outputs=gr.Textbox(),\n",
    "                     title=\"Kiwitech LPU-RAG\",\n",
    "                     description=\"Ask any question about your document, and get an answer along with the response time.\")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b32790c-fe70-40f7-9368-4b978fc35a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
